{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "from pypfopt.risk_models import sample_cov\n",
    "from pypfopt.risk_models import fix_nonpositive_semidefinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quandl\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.expected_returns import returns_from_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading daily quotations of polish companies for the last 10y\n",
    "def get_stock_data(path):\n",
    "    url = [\"https://info.bossa.pl/pub/ciagle/omega/omegacgl.zip\", \"https://info.bossa.pl/pub/newconnect/omega/omegancn.zip\"]\n",
    "    request = []\n",
    "    for i in url:\n",
    "        request.append(requests.get(i, allow_redirects=True))\n",
    "    for i,j in enumerate(request):\n",
    "        with open(path+str(i)+'Stocks.zip', 'wb') as f:\n",
    "            f.write(j.content)\n",
    "        with zipfile.ZipFile(path+str(i)+'Stocks.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(path+'Stocks/')\n",
    "    read_files = glob.glob(path+'Stocks/*.txt')\n",
    "    with open(\"result.txt\", \"wb\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "    data = pd.read_table('result.txt', sep =',', header=0)\n",
    "    data.columns = ['Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    data = data.loc[data['Volume'] != 'Volume']\n",
    "    data = data.loc[data['Volume'] != '<VOL>']\n",
    "    convert_dtypes = {'Volume': float, \n",
    "                      'Close': float,\n",
    "                      'Date':'datetime64', \n",
    "                      'Ticker':'string'}\n",
    "    data = data.astype(convert_dtypes)\n",
    "    data.drop(columns = ['Open', 'High', 'Low'], inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/pawel/PycharmProjects/quandl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_stock_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamentals():\n",
    "    df_companies = pd.read_csv('Companies_gvkey.csv').dropna()\n",
    "    df_fundamentals = pd.read_csv('Polish_Fundamentals_Annual.csv')\n",
    "    df_fundamentals = pd.merge(df_fundamentals,df_companies, on = 'gvkey', how = 'inner')\n",
    "    df_fundamentals.rename(columns = {'fyear':'year'}, inplace = True)\n",
    "    return df_fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fundamentals = get_fundamentals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = pd.read_csv('sectors_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_market_fundamentals(df_fundamentals, data):\n",
    "    df_gvkey = df_fundamentals[['gvkey', 'Ticker']].groupby(['gvkey'])[['Ticker']].last().reset_index()\n",
    "    data_combined = pd.merge(data, df_gvkey, on = 'Ticker', how = 'inner')\n",
    "    return data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = combined_market_fundamentals(df_fundamentals, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating year column based on date column\n",
    "data_combined['year'] = data_combined.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns mean historical return of polish stocks\n",
    "# param year enables to set a year of analysis\n",
    "def annualized_mean_returns(data_combined, year = None):\n",
    "    data_returns = data_combined.copy(deep=False)\n",
    "    if year != None:\n",
    "        data_returns = data_combined.loc[data_combined.year == year]\n",
    "    data_returns = data_returns.groupby(['Ticker', 'Date'])[['Close']].mean().unstack(level=0)\n",
    "    data_returns = mean_historical_return(data_returns, returns_data=False, compounding=True, frequency=252)\n",
    "    data_returns = pd.DataFrame(data_returns).reset_index().drop(columns = ['level_0']).rename(columns = {0:'Return'}).sort_values('Return',ascending = False)\n",
    "    if year != None:\n",
    "        data_returns['year'] = year\n",
    "    return data_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_returns = annualized_mean_returns(data_combined, year = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns defined percentage of mean return distribution\n",
    "# e.g. x = 0.9 means that presented companies returns' are higher than returns of 90% companies in sample\n",
    "def returns_x_quantile(x, data_returns, upper = True):\n",
    "    if upper == True:\n",
    "        data_quantile = data_returns.loc[data_returns.Return >= data_returns.quantile(x)[0]]\n",
    "    else:\n",
    "        data_quantile = data_returns.loc[data_returns.Return <= data_returns.quantile(x)[0]]\n",
    "    return data_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns annualized covariance matrix\n",
    "def annualized_matrix(data_combined, data_returns):\n",
    "    data_matrix = pd.merge(data_combined, data_returns, on = ('Ticker', 'year'), how = 'inner')\n",
    "    annualized_cov_matrix = sample_cov(data_matrix.groupby(['Ticker', 'Date'])[['Close']].mean().unstack(level=0))\n",
    "    annualized_cov_matrix = fix_nonpositive_semidefinite(annualized_cov_matrix)\n",
    "    return annualized_cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns weights of portfolio, which maximizes Sharpe ratio\n",
    "def sharpe_opt(data_returns, annualized_cov_matrix):\n",
    "    ef = EfficientFrontier(data_returns.set_index('Ticker').Return, annualized_cov_matrix)\n",
    "    weights = ef.max_sharpe()\n",
    "    weights = pd.DataFrame.from_dict(weights, orient = 'index').rename(columns = {0:'portfolio_share'}).sort_values('portfolio_share', ascending = False)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualized_cov_matrix = annualized_matrix(data_combined, data_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sharpe_opt(data_returns, annualized_cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting weights in dict\n",
    "# weights = {k: v for k, v in sorted(weights.items(), key=lambda item: item[1], reverse = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.rename(columns = {'index':'Ticker'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_perform_stocks(data_combined, quantile, year = None):\n",
    "    returns_t_0 = annualized_mean_returns(data_combined, year = year)\n",
    "    returns_t_0 = returns_x_quantile(quantile, returns_t_0)\n",
    "    returns_t_1 = annualized_mean_returns(data_combined, year = year+1)\n",
    "    return_comp = pd.merge(returns_t_1, returns_t_0, on = 'Ticker', how = 'inner')\n",
    "    return return_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_perform_stocks(data_combined, quantile, year = None):\n",
    "    returns_t_0 = annualized_mean_returns(data_combined, year = year)\n",
    "    returns_t_0 = returns_x_quantile(quantile, returns_t_0, upper = False)\n",
    "    returns_t_1 = annualized_mean_returns(data_combined, year = year+1)\n",
    "    return_comp = pd.merge(returns_t_1, returns_t_0, on = 'Ticker', how = 'inner')\n",
    "    return return_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trend_reversion(data_combined, percentile, lag_days, freq = 'W', drop = False):\n",
    "    \n",
    "#     prices_df = data_combined.groupby(['Ticker', 'Date'])[['Close']].mean().unstack(level=0)\n",
    "#     prices_freq = returnd_df.groupby(pd.Grouper(level='Date', freq=freq)).mean()\n",
    "#     prices_freq = returns_from_prices(prices_freq, log_returns=True)\n",
    "#     prices_freq = prices_freq.stack(1)[['Close']]\n",
    "    \n",
    "#     if drop == False:\n",
    "#         prices_freq = prices_freq.loc[prices_freq.Close > prices_freq.Close.quantile(percentile)]\n",
    "#     else:\n",
    "#         prices_freq = prices_freq.loc[prices_freq.Close < prices_freq.Close.quantile(percentile)]\n",
    "        \n",
    "#     prices_freq.reset_index(inplace=True)\n",
    "#     df_lagged_combined = data_combined[0:0]\n",
    "#     for i in range(len(prices_freq)):\n",
    "#         df_lagged = data_combined.loc[(data_combined['Ticker'] == prices_freq.iloc[i,:][1]) & \n",
    "#                                       (data_combined['Date'] >= prices_freq.iloc[i,:][0]) & \n",
    "#                                       (data_combined['Date'] <= str(datetime.strptime(str(prices_freq.iloc[i,:][0]), '%Y-%m-%d %H:%M:%S')+timedelta(days=lag_days)))]\n",
    "#         df_lagged_combined = pd.concat([df_lagged_combined,df_lagged])\n",
    "        \n",
    "#     df_lagged_combined = df_lagged_combined.groupby(['Ticker', 'Date'])[['Close']].mean().unstack(level=0)\n",
    "#     df_lagged_combined = returns_from_prices(df_lagged_combined, log_returns=True)\n",
    "#     return df_lagged_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fads hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative long-term serial correlation in the performance of the aggregate market\n",
    "# Function tests “fads hypothesis,” which asserts that the stock market might overreact to recent relevant news \n",
    "\n",
    "def reversal_effect(data_combined, percentile, lag_periods = 4, freq = 'W', drop = False):\n",
    "    \n",
    "    prices_df = data_combined.groupby(['Ticker', 'Date'])[['Close']].mean().unstack(level=0)\n",
    "    prices_freq = prices_df.groupby(pd.Grouper(level='Date', freq=freq)).mean()\n",
    "    returns_freq = returns_from_prices(prices_freq, log_returns=True)\n",
    "    returns_freq = returns_freq.stack(1)[['Close']]\n",
    "    \n",
    "    if drop == False:\n",
    "        returns_freq = returns_freq.loc[returns_freq.Close > returns_freq.Close.quantile(percentile)]\n",
    "    else:\n",
    "        returns_freq = returns_freq.loc[returns_freq.Close < returns_freq.Close.quantile(percentile)]\n",
    "        \n",
    "    prices_stacked = prices_freq.stack(1).rename(columns = {'Close':'Close_lag'})\n",
    "    \n",
    "    prices_combined = prices_stacked.join(returns_freq, on=['Date','Ticker'])\n",
    "    prices_combined = prices_combined.reorder_levels(['Ticker', 'Date'])\n",
    "    prices_combined['Close_lag_2'] = prices_combined.groupby(level=0)[['Close_lag']].shift(-lag_periods)\n",
    "    prices_combined = prices_combined.loc[prices_combined.Close.notna()]\n",
    "    prices_combined['ror_'+freq+str(lag_periods)] = np.log(prices_combined['Close_lag_2']/prices_combined['Close_lag'])\n",
    "    prices_combined.dropna(subset=['ror_'+freq+str(lag_periods)], inplace=True)\n",
    "\n",
    "    return prices_combined[['ror_'+freq+str(lag_periods)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_combined = reversal_effect(data_combined, 0.999, lag_periods = 21, freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ror_D21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EFEKT</th>\n",
       "      <th>1993-08-05</th>\n",
       "      <td>1.044412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-09-16</th>\n",
       "      <td>0.002497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEST</th>\n",
       "      <th>2001-06-08</th>\n",
       "      <td>0.016086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETIA</th>\n",
       "      <th>2001-10-10</th>\n",
       "      <td>0.501937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STALEXP</th>\n",
       "      <th>2001-11-19</th>\n",
       "      <td>-0.317302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REMEDIS</th>\n",
       "      <th>2020-11-19</th>\n",
       "      <td>-0.173847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INWESTPL</th>\n",
       "      <th>2020-11-20</th>\n",
       "      <td>-0.355034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPENFIN</th>\n",
       "      <th>2020-11-20</th>\n",
       "      <td>-0.158748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADVERTIGO</th>\n",
       "      <th>2020-11-23</th>\n",
       "      <td>-0.256296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APIS</th>\n",
       "      <th>2020-11-25</th>\n",
       "      <td>-0.474594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2654 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ror_D21\n",
       "Ticker    Date                \n",
       "EFEKT     1993-08-05  1.044412\n",
       "          1993-09-16  0.002497\n",
       "BEST      2001-06-08  0.016086\n",
       "NETIA     2001-10-10  0.501937\n",
       "STALEXP   2001-11-19 -0.317302\n",
       "...                        ...\n",
       "REMEDIS   2020-11-19 -0.173847\n",
       "INWESTPL  2020-11-20 -0.355034\n",
       "OPENFIN   2020-11-20 -0.158748\n",
       "ADVERTIGO 2020-11-23 -0.256296\n",
       "APIS      2020-11-25 -0.474594\n",
       "\n",
       "[2654 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13520668502161115"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prices_combined.ror_D21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividend yield hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tests if the return on aggregate stock market tends to be higher when the dividend/price ratio, the dividend yield, is high\n",
    "\n",
    "def dividend_price_ratio(data_combined, df_fundamentals, lag_years = 1, percentile = None):\n",
    "    \n",
    "    dv_df = pd.merge(data_combined, df_fundamentals[['gvkey', 'year','dv', 'cshoi']], on = ('gvkey', 'year'), how = 'inner').dropna(subset=['dv', 'cshoi'])\n",
    "    dv_df['dv_p_share'] = dv_df['dv']/dv_df['cshoi']\n",
    "    dv_df = dv_df.groupby(['Ticker', 'year'])[['Close', 'dv_p_share']].mean()\n",
    "    dv_df['dv_yield'] = dv_df['dv_p_share']/dv_df['Close']\n",
    "    \n",
    "    df_price = data_combined.groupby(['Ticker','year'])[['Close']].last()\n",
    "    df_returns = returns_from_prices(df_price.unstack(level=0), log_returns=True)\n",
    "    df_returns = df_returns.stack(1).reorder_levels(['Ticker', 'year']).reset_index()\n",
    "    df_returns = df_returns.groupby(['Ticker', 'year'])[['Close']].mean().shift(-lag_years)\n",
    "    \n",
    "    dv_df = dv_df[['dv_yield']].join(df_returns, on=['Ticker', 'year']).dropna()\n",
    "    \n",
    "    if percentile != None:\n",
    "        dv_df_percentile = dv_df.loc[dv_df.dv_yield > dv_df.dv_yield.quantile(percentile)]\n",
    "    else:\n",
    "        dv_df_percentile = dv_df.copy(deep=False)\n",
    "        \n",
    "    dv_corr_returns = dv_df.dv_yield.corr(dv_df.Close)\n",
    "    \n",
    "    return np.mean(dv_df_percentile.Close), dv_corr_returns, dv_df_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_mean, dv_corr, dv_df= dividend_price_ratio(data_combined, df_fundamentals, lag_years = 1, percentile = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0992796991465278"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07683296163258192"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dv_yield</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4FUNMEDIA</th>\n",
       "      <th>2012</th>\n",
       "      <td>0.100787</td>\n",
       "      <td>-0.082521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.123626</td>\n",
       "      <td>-1.148209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ACARTUS</th>\n",
       "      <th>2015</th>\n",
       "      <td>0.204943</td>\n",
       "      <td>0.087011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.234432</td>\n",
       "      <td>0.117783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.238589</td>\n",
       "      <td>0.075349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">XPLUS</th>\n",
       "      <th>2012</th>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.087011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.136680</td>\n",
       "      <td>1.007958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZAMET</th>\n",
       "      <th>2012</th>\n",
       "      <td>0.182003</td>\n",
       "      <td>0.711839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEPAK</th>\n",
       "      <th>2018</th>\n",
       "      <td>0.120733</td>\n",
       "      <td>0.026317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYWIEC</th>\n",
       "      <th>2010</th>\n",
       "      <td>0.097408</td>\n",
       "      <td>-0.094427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                dv_yield     Close\n",
       "Ticker    year                    \n",
       "4FUNMEDIA 2012  0.100787 -0.082521\n",
       "          2013  0.123626 -1.148209\n",
       "ACARTUS   2015  0.204943  0.087011\n",
       "          2016  0.234432  0.117783\n",
       "          2018  0.238589  0.075349\n",
       "...                  ...       ...\n",
       "XPLUS     2012  0.110694  0.087011\n",
       "          2019  0.136680  1.007958\n",
       "ZAMET     2012  0.182003  0.711839\n",
       "ZEPAK     2018  0.120733  0.026317\n",
       "ZYWIEC    2010  0.097408 -0.094427\n",
       "\n",
       "[171 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earnings yield hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tests if the return on aggregate stock market tends to be higher when the earnings yield is high\n",
    "\n",
    "def earnings_yield(data_combined, df_fundamentals, percentile = None):\n",
    "    \n",
    "    ib_df = pd.merge(data_combined, df_fundamentals[['gvkey', 'year','ib', 'cshoi']], on = ('gvkey', 'year'), how = 'inner').dropna(subset=['ib', 'cshoi'])\n",
    "    ib_df['ib_p_share'] = ib_df['ib']/ib_df['cshoi']\n",
    "    ib_df = ib_df.groupby(['Ticker', 'year'])[['Close', 'ib_p_share']].last()\n",
    "    ib_df['ib_yield'] = ib_df['ib_p_share']/ib_df['Close']\n",
    "    \n",
    "    df_price = data_combined.groupby(['Ticker','year'])[['Close']].last()\n",
    "    df_returns = returns_from_prices(df_price.unstack(level=0), log_returns=True)\n",
    "    df_returns = df_returns.stack(1).reorder_levels(['Ticker', 'year']).reset_index()\n",
    "    df_returns = df_returns.groupby(['Ticker', 'year'])[['Close']].mean().shift(-1)\n",
    "    \n",
    "    ib_df = ib_df[['ib_yield']].join(df_returns, on=['Ticker', 'year']).dropna()\n",
    "    \n",
    "    if percentile != None:\n",
    "        ib_df_percentile = ib_df.loc[ib_df.ib_yield > ib_df.ib_yield.quantile(percentile)]\n",
    "    else:\n",
    "        ib_df_percentile = ib_df.copy(deep=False)\n",
    "        \n",
    "    ib_corr_returns = ib_df.ib_yield.corr(ib_df.Close)\n",
    "    \n",
    "    return np.mean(ib_df_percentile.Close), ib_corr_returns, ib_df_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "earn_mean,earn_corr,earn_df = earnings_yield(data_combined, df_fundamentals, percentile = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15431902676585418"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earn_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009397600208458236"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earn_corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
